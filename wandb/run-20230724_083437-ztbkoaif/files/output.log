Age distribution:  count    25656.000000
mean        60.416004
std          7.382716
min         44.000000
25%         54.750000
50%         61.000000
75%         66.000000
max         77.000000
Name: age, dtype: float64
25656
images_names:  19242 23086
labels:  19242 57.0
images_names:  6414 22671
labels:  6414 75.0
/home/n1/yeinhwang/anaconda3/envs/ba_torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv3d-1    [-1, 32, 128, 128, 128]             896
              ReLU-2    [-1, 32, 128, 128, 128]               0
       BatchNorm3d-3    [-1, 32, 128, 128, 128]              64
            Conv3d-4    [-1, 32, 128, 128, 128]          27,680
              ReLU-5    [-1, 32, 128, 128, 128]               0
         Dropout3d-6    [-1, 32, 128, 128, 128]               0
         MaxPool3d-7       [-1, 32, 64, 64, 64]               0
            Conv3d-8       [-1, 64, 64, 64, 64]          55,360
              ReLU-9       [-1, 64, 64, 64, 64]               0
      BatchNorm3d-10       [-1, 64, 64, 64, 64]             128
           Conv3d-11       [-1, 64, 64, 64, 64]         110,656
             ReLU-12       [-1, 64, 64, 64, 64]               0
        Dropout3d-13       [-1, 64, 64, 64, 64]               0
        MaxPool3d-14       [-1, 64, 32, 32, 32]               0
           Conv3d-15       [-1, 64, 32, 32, 32]         110,656
             ReLU-16       [-1, 64, 32, 32, 32]               0
      BatchNorm3d-17       [-1, 64, 32, 32, 32]             128
           Conv3d-18       [-1, 64, 32, 32, 32]         110,656
             ReLU-19       [-1, 64, 32, 32, 32]               0
      BatchNorm3d-20       [-1, 64, 32, 32, 32]             128
        MaxPool3d-21       [-1, 64, 16, 16, 16]               0
           Conv3d-22       [-1, 64, 16, 16, 16]         110,656
             ReLU-23       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-24       [-1, 64, 16, 16, 16]             128
           Conv3d-25       [-1, 64, 16, 16, 16]         110,656
             ReLU-26       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-27       [-1, 64, 16, 16, 16]             128
           Conv3d-28       [-1, 64, 16, 16, 16]         110,656
             ReLU-29       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-30       [-1, 64, 16, 16, 16]             128
        MaxPool3d-31          [-1, 64, 8, 8, 8]               0
           Conv3d-32          [-1, 96, 8, 8, 8]         165,984
             ReLU-33          [-1, 96, 8, 8, 8]               0
      BatchNorm3d-34          [-1, 96, 8, 8, 8]             192
           Conv3d-35          [-1, 96, 8, 8, 8]         248,928
             ReLU-36          [-1, 96, 8, 8, 8]               0
      BatchNorm3d-37          [-1, 96, 8, 8, 8]             192
           Conv3d-38          [-1, 96, 8, 8, 8]         248,928
             ReLU-39          [-1, 96, 8, 8, 8]               0
      BatchNorm3d-40          [-1, 96, 8, 8, 8]             192
        MaxPool3d-41          [-1, 96, 4, 4, 4]               0
           Conv3d-42          [-1, 96, 4, 4, 4]         248,928
             ReLU-43          [-1, 96, 4, 4, 4]               0
      BatchNorm3d-44          [-1, 96, 4, 4, 4]             192
           Conv3d-45          [-1, 96, 4, 4, 4]         248,928
             ReLU-46          [-1, 96, 4, 4, 4]               0
      BatchNorm3d-47          [-1, 96, 4, 4, 4]             192
           Conv3d-48          [-1, 96, 4, 4, 4]         248,928
             ReLU-49          [-1, 96, 4, 4, 4]               0
      BatchNorm3d-50          [-1, 96, 4, 4, 4]             192
        MaxPool3d-51          [-1, 96, 2, 2, 2]               0
           Conv3d-52          [-1, 96, 2, 2, 2]         248,928
             ReLU-53          [-1, 96, 2, 2, 2]               0
      BatchNorm3d-54          [-1, 96, 2, 2, 2]             192
           Conv3d-55          [-1, 96, 2, 2, 2]         248,928
             ReLU-56          [-1, 96, 2, 2, 2]               0
      BatchNorm3d-57          [-1, 96, 2, 2, 2]             192
           Conv3d-58          [-1, 96, 2, 2, 2]         248,928
             ReLU-59          [-1, 96, 2, 2, 2]               0
      BatchNorm3d-60          [-1, 96, 2, 2, 2]             192
          Flatten-61                  [-1, 768]               0
           Linear-62                   [-1, 96]          73,824
             ReLU-63                   [-1, 96]               0
           Linear-64                   [-1, 32]           3,104
             ReLU-65                   [-1, 32]               0
           Linear-66                    [-1, 1]              33
              CNN-67                    [-1, 1]               0
           Conv3d-68    [-1, 32, 128, 128, 128]             896
             ReLU-69    [-1, 32, 128, 128, 128]               0
      BatchNorm3d-70    [-1, 32, 128, 128, 128]              64
           Conv3d-71    [-1, 32, 128, 128, 128]          27,680
             ReLU-72    [-1, 32, 128, 128, 128]               0
        Dropout3d-73    [-1, 32, 128, 128, 128]               0
        MaxPool3d-74       [-1, 32, 64, 64, 64]               0
           Conv3d-75       [-1, 64, 64, 64, 64]          55,360
             ReLU-76       [-1, 64, 64, 64, 64]               0
      BatchNorm3d-77       [-1, 64, 64, 64, 64]             128
           Conv3d-78       [-1, 64, 64, 64, 64]         110,656
             ReLU-79       [-1, 64, 64, 64, 64]               0
        Dropout3d-80       [-1, 64, 64, 64, 64]               0
        MaxPool3d-81       [-1, 64, 32, 32, 32]               0
           Conv3d-82       [-1, 64, 32, 32, 32]         110,656
             ReLU-83       [-1, 64, 32, 32, 32]               0
      BatchNorm3d-84       [-1, 64, 32, 32, 32]             128
           Conv3d-85       [-1, 64, 32, 32, 32]         110,656
             ReLU-86       [-1, 64, 32, 32, 32]               0
      BatchNorm3d-87       [-1, 64, 32, 32, 32]             128
        MaxPool3d-88       [-1, 64, 16, 16, 16]               0
           Conv3d-89       [-1, 64, 16, 16, 16]         110,656
             ReLU-90       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-91       [-1, 64, 16, 16, 16]             128
           Conv3d-92       [-1, 64, 16, 16, 16]         110,656
             ReLU-93       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-94       [-1, 64, 16, 16, 16]             128
           Conv3d-95       [-1, 64, 16, 16, 16]         110,656
             ReLU-96       [-1, 64, 16, 16, 16]               0
      BatchNorm3d-97       [-1, 64, 16, 16, 16]             128
        MaxPool3d-98          [-1, 64, 8, 8, 8]               0
           Conv3d-99          [-1, 96, 8, 8, 8]         165,984
            ReLU-100          [-1, 96, 8, 8, 8]               0
     BatchNorm3d-101          [-1, 96, 8, 8, 8]             192
          Conv3d-102          [-1, 96, 8, 8, 8]         248,928
            ReLU-103          [-1, 96, 8, 8, 8]               0
     BatchNorm3d-104          [-1, 96, 8, 8, 8]             192
          Conv3d-105          [-1, 96, 8, 8, 8]         248,928
            ReLU-106          [-1, 96, 8, 8, 8]               0
     BatchNorm3d-107          [-1, 96, 8, 8, 8]             192
       MaxPool3d-108          [-1, 96, 4, 4, 4]               0
          Conv3d-109          [-1, 96, 4, 4, 4]         248,928
            ReLU-110          [-1, 96, 4, 4, 4]               0
     BatchNorm3d-111          [-1, 96, 4, 4, 4]             192
          Conv3d-112          [-1, 96, 4, 4, 4]         248,928
            ReLU-113          [-1, 96, 4, 4, 4]               0
     BatchNorm3d-114          [-1, 96, 4, 4, 4]             192
          Conv3d-115          [-1, 96, 4, 4, 4]         248,928
            ReLU-116          [-1, 96, 4, 4, 4]               0
     BatchNorm3d-117          [-1, 96, 4, 4, 4]             192
       MaxPool3d-118          [-1, 96, 2, 2, 2]               0
          Conv3d-119          [-1, 96, 2, 2, 2]         248,928
            ReLU-120          [-1, 96, 2, 2, 2]               0
     BatchNorm3d-121          [-1, 96, 2, 2, 2]             192
          Conv3d-122          [-1, 96, 2, 2, 2]         248,928
            ReLU-123          [-1, 96, 2, 2, 2]               0
     BatchNorm3d-124          [-1, 96, 2, 2, 2]             192
          Conv3d-125          [-1, 96, 2, 2, 2]         248,928
            ReLU-126          [-1, 96, 2, 2, 2]               0
     BatchNorm3d-127          [-1, 96, 2, 2, 2]             192
         Flatten-128                  [-1, 768]               0
          Linear-129                   [-1, 96]          73,824
            ReLU-130                   [-1, 96]               0
          Linear-131                   [-1, 32]           3,104
            ReLU-132                   [-1, 32]               0
          Linear-133                    [-1, 1]              33
             CNN-134                    [-1, 1]               0
================================================================
Total params: 5,969,602
Trainable params: 5,969,602
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 8.00
Forward/backward pass size (MB): 8080.32
Params size (MB): 22.77
Estimated Total Size (MB): 8111.09
----------------------------------------------------------------
[ Start ]
Learning rate loading:  1.479507962909596e-05
Loaded model: model/cust_32/3d_cnn-11.pth.tar
Epoch  12: training
































































































































































































































































































































































































































































































































































100%|██████████| 602/602 [2:05:55<00:00, 12.55s/it]
  0%|          | 0/201 [00:00<?, ?it/s]
Epoch: 12, duration for training: 125.95 minutes

























































100%|█████████▉| 200/201 [30:44<00:01,  1.58s/it]
    Epoch 12: training mse loss = 21.988 / validation mse loss = 19.180
    Epoch 12: training mae loss = 3.746 / validation mae loss = 3.450
100%|██████████| 201/201 [30:48<00:00,  9.20s/it]
  0%|          | 0/602 [00:00<?, ?it/s]
    Best Saved model: best-model/cust_32-12.pth.tar
Epoch: 13, duration for validation: 30.82 minutes






































































































































































































































































































































































































































































































































100%|█████████▉| 601/602 [1:30:45<00:04,  4.57s/it]
Epoch: 13, duration for training: 121.60 minutes
100%|██████████| 602/602 [1:30:46<00:00,  9.05s/it]
















































100%|██████████| 201/201 [27:08<00:00,  1.66s/it]
    Epoch 13: training mse loss = 13.112 / validation mse loss = 16.722
    Epoch 13: training mae loss = 2.849 / validation mae loss = 3.227
>>>>>>>>>>>>>>>>>> Loss updates
    Best Saved model: best-model/cust_32-13.pth.tar
Epoch: 14, duration for validation: 27.16 minutes
100%|██████████| 201/201 [27:09<00:00,  8.11s/it]





























































































































































































































































































































































































































































































































































100%|██████████| 602/602 [1:54:52<00:00,  4.27s/it]
Epoch: 14, duration for training: 142.03 minutes
100%|██████████| 602/602 [1:54:52<00:00, 11.45s/it]
































































































































100%|█████████▉| 200/201 [45:21<00:04,  4.33s/it]
    Epoch 14: training mse loss = 11.481 / validation mse loss = 17.599
100%|██████████| 201/201 [45:22<00:00, 13.54s/it]
  0%|          | 0/602 [00:00<?, ?it/s]
Epoch: 15, duration for validation: 45.37 minutes































































































































































































































































































